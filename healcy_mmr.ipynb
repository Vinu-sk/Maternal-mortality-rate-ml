{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36173812  0.91339632  0.25502279  1.90589019 -0.4852155   1.44695615]\n",
      " [ 0.38077697  1.45702716  0.97553854  1.29833966 -0.4852155  -0.53208757]\n",
      " [-0.06473208 -1.26112705 -0.46549297 -0.22053665  0.97388449  0.70481475]\n",
      " ...\n",
      " [ 0.38077697 -1.53294248 -1.18600873  3.12099124 -0.4852155   1.44695615]\n",
      " [ 0.97478904  0.36976548  0.97553854  2.81721597 -0.4852155  -0.53208757]\n",
      " [ 0.15802244  0.36976548 -0.82575085 -0.82808717  1.70343448  0.21005383]]\n",
      "0       2\n",
      "1       2\n",
      "2       2\n",
      "3       2\n",
      "4       0\n",
      "       ..\n",
      "1009    2\n",
      "1010    2\n",
      "1011    2\n",
      "1012    2\n",
      "1013    1\n",
      "Name: RiskLevel, Length: 1014, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(r\"C:\\Users\\jason\\OneDrive\\Desktop\\healcy\\archive\\test_data.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('RiskLevel', axis=1)\n",
    "y = data['RiskLevel']\n",
    "\n",
    "# Map 'RiskLevel' labels to numerical values\n",
    "label_mapping = {'low risk': 0, 'mid risk': 1, 'high risk': 2}\n",
    "y = y.map(label_mapping)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# TensorFlow model\n",
    "tf_model = Sequential()\n",
    "tf_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "tf_model.add(Dense(128, activation='relu'))\n",
    "tf_model.add(Dense(128, activation='relu'))\n",
    "tf_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile TensorFlow model\n",
    "tf_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "26/26 [==============================] - 1s 2ms/step - loss: 0.9034 - accuracy: 0.6178\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.7248 - accuracy: 0.6880\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.6794\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.7078\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.6188 - accuracy: 0.7176\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.6129 - accuracy: 0.7090\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7016\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.7226\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5702 - accuracy: 0.7337\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.7349\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5476 - accuracy: 0.7374\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7559\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.7300\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7435\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7460\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7398\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5105 - accuracy: 0.7485\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5125 - accuracy: 0.7497\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 0s 818us/step - loss: 0.5090 - accuracy: 0.7448\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.7583\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.7583\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.7583\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7682\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7682\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 0s 990us/step - loss: 0.4727 - accuracy: 0.7620\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 0s 970us/step - loss: 0.4617 - accuracy: 0.7768\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 0s 941us/step - loss: 0.4663 - accuracy: 0.7867\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7904\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.7978\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.7904\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 0s 929us/step - loss: 0.4474 - accuracy: 0.7818\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 0s 973us/step - loss: 0.4461 - accuracy: 0.7818\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.7916\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 0s 884us/step - loss: 0.4218 - accuracy: 0.7941\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.7978\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 0s 984us/step - loss: 0.4340 - accuracy: 0.7793\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 0s 981us/step - loss: 0.4324 - accuracy: 0.7916\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4323 - accuracy: 0.8027\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 0s 899us/step - loss: 0.4145 - accuracy: 0.8187\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4020 - accuracy: 0.8187\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8200\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8298\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 0s 974us/step - loss: 0.4087 - accuracy: 0.8150\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 0s 931us/step - loss: 0.3989 - accuracy: 0.8052\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 0s 934us/step - loss: 0.4105 - accuracy: 0.8237\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 0s 896us/step - loss: 0.4062 - accuracy: 0.8101\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 0s 981us/step - loss: 0.3834 - accuracy: 0.8323\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3837 - accuracy: 0.8298\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 0s 896us/step - loss: 0.3844 - accuracy: 0.8360\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 0s 891us/step - loss: 0.3877 - accuracy: 0.8224\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 0s 932us/step - loss: 0.3782 - accuracy: 0.8298\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 0s 876us/step - loss: 0.3761 - accuracy: 0.8323\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 0s 984us/step - loss: 0.4021 - accuracy: 0.8237\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 0s 925us/step - loss: 0.3879 - accuracy: 0.8323\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 0s 851us/step - loss: 0.3633 - accuracy: 0.8434\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 0s 946us/step - loss: 0.3681 - accuracy: 0.8372\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 0s 941us/step - loss: 0.3706 - accuracy: 0.8348\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 0s 887us/step - loss: 0.3667 - accuracy: 0.8237\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 0s 912us/step - loss: 0.3548 - accuracy: 0.8471\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 0s 847us/step - loss: 0.3519 - accuracy: 0.8459\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 0s 920us/step - loss: 0.3657 - accuracy: 0.8298\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 0s 520us/step - loss: 0.3506 - accuracy: 0.8422\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 0s 978us/step - loss: 0.3320 - accuracy: 0.8520\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 0s 870us/step - loss: 0.3525 - accuracy: 0.8483\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 0s 900us/step - loss: 0.3615 - accuracy: 0.8372\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 0s 920us/step - loss: 0.3483 - accuracy: 0.8409\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 0s 926us/step - loss: 0.3411 - accuracy: 0.8483\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 0s 906us/step - loss: 0.3351 - accuracy: 0.8483\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 0s 929us/step - loss: 0.3416 - accuracy: 0.8446\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 0s 989us/step - loss: 0.3511 - accuracy: 0.8360\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 0s 859us/step - loss: 0.3401 - accuracy: 0.8397\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 0s 899us/step - loss: 0.3681 - accuracy: 0.8496\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 0s 923us/step - loss: 0.3337 - accuracy: 0.8459\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 0s 964us/step - loss: 0.3221 - accuracy: 0.8582\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 0s 895us/step - loss: 0.3296 - accuracy: 0.8471\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 0s 972us/step - loss: 0.3308 - accuracy: 0.8459\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 0s 909us/step - loss: 0.3113 - accuracy: 0.8631\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 0s 776us/step - loss: 0.3295 - accuracy: 0.8557\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 0s 961us/step - loss: 0.3379 - accuracy: 0.8348\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.8533\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 0s 889us/step - loss: 0.3171 - accuracy: 0.8545\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 0s 961us/step - loss: 0.3046 - accuracy: 0.8693\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 0s 809us/step - loss: 0.2983 - accuracy: 0.8644\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 0s 925us/step - loss: 0.3142 - accuracy: 0.8570\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 0s 912us/step - loss: 0.3167 - accuracy: 0.8693\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3025 - accuracy: 0.8718\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8631\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3038 - accuracy: 0.8631\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3162 - accuracy: 0.8520\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8656\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8644\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8681\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2912 - accuracy: 0.8681\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8742\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.8718\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8718\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2989 - accuracy: 0.8792\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 0s 771us/step - loss: 0.2886 - accuracy: 0.8644\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2912 - accuracy: 0.8619\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2868 - accuracy: 0.8705\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 0s 956us/step - loss: 0.2824 - accuracy: 0.8804\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8742\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.8668\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8681\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.8742\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.8829\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.8779\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2848 - accuracy: 0.8779\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8841\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8792\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2745 - accuracy: 0.8779\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 0s 966us/step - loss: 0.2752 - accuracy: 0.8804\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8582\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8705\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8755\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8866\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 0s 862us/step - loss: 0.2919 - accuracy: 0.8668\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.8681\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8755\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2688 - accuracy: 0.8742\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 0s 946us/step - loss: 0.2657 - accuracy: 0.8878\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8866\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.8866\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 0s 894us/step - loss: 0.2800 - accuracy: 0.8718\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8853\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2540 - accuracy: 0.8779\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2559 - accuracy: 0.8829\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2537 - accuracy: 0.8804\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 0s 992us/step - loss: 0.2608 - accuracy: 0.8853\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2649 - accuracy: 0.8816\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 0s 789us/step - loss: 0.2506 - accuracy: 0.8718\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2763 - accuracy: 0.8631\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8742\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 0s 965us/step - loss: 0.2617 - accuracy: 0.8816\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8779\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2490 - accuracy: 0.8890\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 0s 935us/step - loss: 0.2506 - accuracy: 0.8890\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.8890\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.8853\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.8841\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.8866\n",
      "Epoch 142/150\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1960 - accuracy: 0.9062"
     ]
    }
   ],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Train TensorFlow model\n",
    "tf_model.fit(X_train, y_train, epochs=150, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XGBoost predictions\n",
    "xgb_preds = xgb_model.predict(X_val)\n",
    "\n",
    "# Generate TensorFlow predictions\n",
    "tf_probs = tf_model.predict(X_val)\n",
    "tf_preds = np.round(tf_probs).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Perform grid search for hyperparameter tuning\u001b[39;00m\n\u001b[0;32m     11\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(meta_model, params, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(meta_X, y_val)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Get the best meta model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m best_meta_model \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Concatenate base model predictions with the original features\n",
    "meta_X = np.concatenate((X_val, xgb_preds.reshape(-1, 1), tf_preds), axis=1)\n",
    "\n",
    "# Build XGBoost meta model\n",
    "meta_model = xgb.XGBClassifier()\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "params = {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01], 'n_estimators': [100, 200, 300]}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(meta_model, params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(meta_X, y_val)\n",
    "\n",
    "# Get the best meta model\n",
    "best_meta_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the meta model\n",
    "meta_preds = best_meta_model.predict(meta_X)\n",
    "\n",
    "# Evaluate the meta model\n",
    "print(classification_report(y_val, meta_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TensorFlow model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Save the TensorFlow model to H5 format\n",
    "tf_model.save('model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
